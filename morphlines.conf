# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
# Copyright 2014 Cloudera Inc.

# Specify server locations in a SOLR_LOCATOR variable;
# used later in variable substitutions
ZK_HOST : "127.0.0.1:2181/solr"
ZK_HOST : ${?ENV_ZK_HOST}

SOLR_LOCATOR : {
    # Name of solr collection
    collection : enron-email-collection
    # ZooKeeper ensemble
    zkHost : ${ZK_HOST}
}

# Specify an array of one or more morphlines, each of which defines an ETL
# transformation chain. A morphline consists of one or more (potentially
# nested) commands. A morphline is a way to consume records (e.g. Flume events,
# HDFS files or blocks), turn them into a stream of records, and pipe the stream
# of records through a set of easily configurable transformations on it's way to
# Solr (or a MapReduceIndexerTool RecordWriter that feeds via a Reducer into Solr).
morphlines : [
{
    # Name used to identify a morphline. E.g. used if there are multiple morphlines in a
    # morphline config file
    id : morphline1
    # Import all morphline commands in these java packages and their subpackages.
    # Other commands that may be present on the classpath are not visible to this morphline.
    importCommands : ["org.kitesdk.**", "org.apache.solr.**"]
    commands : [
    {
        ## Read the email stream and break it up into individual messages.
        ## The beginning of a message is marked by regex clause below
        ## The reason we use this command is that one event can have multiple
        ## messages
        readMultiLine {
            regex : "Message-ID:.*"
            what : next
            charset : UTF-8
        }
    }
    ## Break up the email text into SOLR fields
    {
        if {
            conditions: [
            {
                not{
                    grok {
                        expressions : {
                            message: """(?s)(.*?)(Message-ID: <)(?<message_id>(.+?))(>.*?)(Date: )(?<date>(.+?))( \(.+?)(From: )(?<from>(.*?))((To: )(?<to>(.+?)))?(Subject: )(?<subject>(.*?))((Cc: )(?<cc>(.*)))?(Mime.+?)((Bcc: )(?<bcc>(.*)))?(X-From: )(?<from_names>(.*?))(X-To: )(?<to_names>(.*?))(X-cc: )(?<cc_names>(.*?))(X-bcc: )(?<bcc_names>(.*?))(X-Folder: )(?<x_folder>(.*?))(X-Origin: )(?<x_origin>(.*?))(X-FileName: )(?<x_filename>(.*?))(\n)(?<body>(.*))"""
                        }
                        extract: inplace
                        findSubstrings: false
                        addEmptyStrings: false
                        numRequiredMatches: all
                    }
                }
            }
            ]
            then:[
            { logInfo { format : "found no grok match: {}", args : ["@{}"] } }
            { dropRecord {} }
            ]
        }
    }

    # add Unique ID, in case our message_id field from above is not present
    {
        generateUUID {
            field:message_id
        }
    }

    # convert the timestamp field to "yyyy-MM-dd'T'HH:mm:ss.SSSZ" format
    {
        convertTimestamp {
            field : date
            inputFormats : ["EEE, d MMM yyyy HH:mm:ss Z", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", "yyyy-MM-dd'T'HH:mm:ss", "yyyy-MM-dd"]
            inputTimezone : America/Los_Angeles
            outputFormat : "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"
            outputTimezone : UTC
        }
    }

    # Consume the output record of the previous command and pipe another
    # record downstream.
    #
    # This command sanitizes record fields that are unknown to Solr schema.xml
    # by deleting them. Recall that Solr throws an exception on any attempt to
    # load a document that contains a field that isn't specified in schema.xml
    {
        sanitizeUnknownSolrFields {
            # Location from which to fetch Solr schema
            solrLocator : ${SOLR_LOCATOR}
        }
    }

    # load the record into a SolrServer or MapReduce SolrOutputFormat.
    {
        loadSolr {
            solrLocator : ${SOLR_LOCATOR}
        }
    }
    ]
}
]
